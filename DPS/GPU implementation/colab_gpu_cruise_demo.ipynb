{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDGK2RdMqbEg"
   },
   "source": [
    "# Cruise Control GPU (Live Mirror) and Intruder allocation\n"
   ],
   "id": "iDGK2RdMqbEg"
  },
  {
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "def has_nvidia_smi():\n",
    "  try:\n",
    "    subprocess.check_output([\"bash\", \"-lc\", \"command -v nvidia-smi\"])\n",
    "    return True\n",
    "  except subprocess.CalledProcessError:\n",
    "    return False\n",
    "\n",
    "GPU_AVAILABLE = has_nvidia_smi()\n",
    "print(\"GPU available:\", GPU_AVAILABLE)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "  !nvidia-smi\n",
    "  !pip -q install cupy-cuda12x\n",
    "else:\n",
    "  print(\"No GPU detected. (Colab: Runtime -> Change runtime type -> GPU)\")\n",
    "  print(\"Will run in CPU mode (NumPy fallback).\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trYo70e7sjqR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770075320286,
     "user_tz": -60,
     "elapsed": 2954,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    },
    "outputId": "5a00197d-811b-4737-9955-12aaa5ee6ec8"
   },
   "id": "trYo70e7sjqR",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU available: True\n",
      "Mon Feb  2 23:35:17 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P0             31W /   70W |     140MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9p2BfeRqbEp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770075320303,
     "user_tz": -60,
     "elapsed": 20,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    },
    "outputId": "7e22d0a9-e69c-4cda-8eea-79742fa9d118"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch, time, requests\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# Match C constants\n",
    "SIM_DT = 0.1\n",
    "TARGET_GAP = 10.0\n",
    "Kp = 0.35\n",
    "Kd = 0.70\n"
   ],
   "id": "w9p2BfeRqbEp"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pdi_Iq1iqbEv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770075320307,
     "user_tz": -60,
     "elapsed": 2,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    }
   },
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def compute_block(sample, device):\n",
    "    # Required fields from latest.json\n",
    "    row = [[\n",
    "        float(sample['my_x']), float(sample['my_y']),\n",
    "        float(sample['leader_x']), float(sample['leader_y']),\n",
    "        float(sample['current_speed']), float(sample['leader_speed']), float(sample['leader_speed'])\n",
    "    ]]\n",
    "\n",
    "    d = torch.tensor(row, dtype=torch.float32, device=device)\n",
    "    my_x, my_y, front_x, front_y, cur_spd, front_spd, leader_spd = d.T\n",
    "\n",
    "    gap = torch.sqrt((my_x - front_x)**2 + (my_y - front_y)**2)\n",
    "    projected_error = (gap - TARGET_GAP) - (front_spd * SIM_DT)\n",
    "\n",
    "    base = leader_spd\n",
    "    damping = (front_spd - cur_spd) * Kd\n",
    "    correction = projected_error * Kp\n",
    "\n",
    "    new_speed = base + damping + correction\n",
    "    new_speed = torch.maximum(new_speed, torch.zeros_like(base))\n",
    "    new_speed = torch.minimum(new_speed, base + 25.0)\n",
    "\n",
    "    return float(gap.item()), float(new_speed.item())\n"
   ],
   "id": "Pdi_Iq1iqbEv"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WAtYH3BLqbEw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770075320321,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Rajdeep\n",
    "# Intruder GPU model (1000x1000 grid)\n",
    "GRID_SIZE = 1000\n",
    "GRID_OFFSET = GRID_SIZE // 2\n",
    "INTRUDER_RADIUS = 4\n",
    "INTRUDER_PROB = 0.05\n",
    "INTRUDER_SPEED = 40\n",
    "INTRUDER_LENGTH = 10\n",
    "INTRUDER_DURATION_MS = 5000\n",
    "\n",
    "intruder_grid = torch.zeros((GRID_SIZE, GRID_SIZE), device=device)\n",
    "\n",
    "def _grid_idx(v):\n",
    "    return max(0, min(GRID_SIZE - 1, int(round(v)) + GRID_OFFSET))\n",
    "\n",
    "def gpu_intruder_scan(sample):\n",
    "    intruder_grid.zero_()\n",
    "    fx = sample['my_x']\n",
    "    fy = sample['my_y']\n",
    "    fx_i = _grid_idx(fx)\n",
    "    fy_i = _grid_idx(fy)\n",
    "\n",
    "    # Spawn near truck with small probability\n",
    "    if torch.rand((), device=device).item() > INTRUDER_PROB:\n",
    "        return False\n",
    "\n",
    "    ox = int(torch.randint(-INTRUDER_RADIUS, INTRUDER_RADIUS + 1, (1,), device=device).item())\n",
    "    oy = int(torch.randint(-INTRUDER_RADIUS, INTRUDER_RADIUS + 1, (1,), device=device).item())\n",
    "    ix = max(0, min(GRID_SIZE - 1, fx_i + ox))\n",
    "    iy = max(0, min(GRID_SIZE - 1, fy_i + oy))\n",
    "\n",
    "    intruder_grid[ix, iy] = 1.0\n",
    "\n",
    "    x0 = max(0, fx_i - INTRUDER_RADIUS)\n",
    "    x1 = min(GRID_SIZE, fx_i + INTRUDER_RADIUS + 1)\n",
    "    y0 = max(0, fy_i - INTRUDER_RADIUS)\n",
    "    y1 = min(GRID_SIZE, fy_i + INTRUDER_RADIUS + 1)\n",
    "\n",
    "    hit = intruder_grid[x0:x1, y0:y1].sum() > 0\n",
    "    return bool(hit)\n"
   ],
   "id": "WAtYH3BLqbEw"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHzjFOwdqbEx"
   },
   "source": [
    "## Live URL\n",
    "Paste your tunnel URL here (cloudflared).\n",
    "Example: `https://abc.trycloudflare.com/latest.json`\n"
   ],
   "id": "sHzjFOwdqbEx"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ih-cJ4YJqbEy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770075338510,
     "user_tz": -60,
     "elapsed": 9,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    }
   },
   "execution_count": 17,
   "outputs": [],
   "source": [
    "LIVE_URL = \"https://overhead-coding-cameron-nova.trycloudflare.com/latest.json\"\n",
    "#need to change during demonstration\n"
   ],
   "id": "Ih-cJ4YJqbEy"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cSAsHx6yqbEz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770075320362,
     "user_tz": -60,
     "elapsed": 3,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    }
   },
   "outputs": [],
   "source": [
    "INTRUDER_POST_URL = LIVE_URL.rsplit('/', 1)[0] + '/intruder'\n"
   ],
   "id": "cSAsHx6yqbEz"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "_cdBorjUqbE0",
    "executionInfo": {
     "status": "error",
     "timestamp": 1770075328401,
     "user_tz": -60,
     "elapsed": 8028,
     "user": {
      "displayName": "Rajdeep Shaw",
      "userId": "07201903206347945809"
     }
    },
    "outputId": "4334958c-a378-4067-e0d6-5396bf16e1cb"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n",
      "Waiting for live JSON...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2275832215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Waiting for live JSON...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Rajdeep\n",
    "seq = 0\n",
    "null_shown = {}\n",
    "for _ in range(2000):\n",
    "    try:\n",
    "        r = requests.get(LIVE_URL, timeout=5)\n",
    "        if r.status_code != 200 or not r.text.strip():\n",
    "            print('Waiting for live JSON...')\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        s = r.json()\n",
    "    except Exception as e:\n",
    "        print('Waiting for live JSON...', e)\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "\n",
    "    samples = []\n",
    "    if isinstance(s, dict) and 'followers' in s:\n",
    "        samples = list(s.get('followers', {}).values())\n",
    "    else:\n",
    "        samples = [s]\n",
    "\n",
    "    if samples and all(sample.get('disconnected') for sample in samples):\n",
    "        print('All followers disconnected. Stopping GPU loop.')\n",
    "        for sample in samples:\n",
    "            fid = int(sample.get('follower_id', 0))\n",
    "            seq += 1\n",
    "            try:\n",
    "                requests.post(INTRUDER_POST_URL, json={\n",
    "                    'seq': seq,\n",
    "                    'target_id': fid,\n",
    "                    'active': False,\n",
    "                    'speed': 0,\n",
    "                    'length': 0,\n",
    "                    'duration_ms': 0,\n",
    "                }, timeout=3)\n",
    "            except Exception:\n",
    "                pass\n",
    "        break\n",
    "\n",
    "    for sample in samples:\n",
    "        fid = int(sample.get('follower_id', 0))\n",
    "\n",
    "        if sample.get('disconnected'):\n",
    "            if not null_shown.get(fid):\n",
    "                print(f'NULL (leader disconnected) fid={fid}')\n",
    "                null_shown[fid] = True\n",
    "            seq += 1\n",
    "            try:\n",
    "                requests.post(INTRUDER_POST_URL, json={\n",
    "                    'seq': seq,\n",
    "                    'target_id': fid,\n",
    "                    'active': False,\n",
    "                    'speed': 0,\n",
    "                    'length': 0,\n",
    "                    'duration_ms': 0,\n",
    "                }, timeout=3)\n",
    "            except Exception:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        null_shown[fid] = False\n",
    "        out = compute_block(sample, device=device)\n",
    "        if out is None:\n",
    "            print('Skipping sample: compute_block returned None', sample)\n",
    "            continue\n",
    "        gap, spd = out\n",
    "\n",
    "        active = gpu_intruder_scan(sample)\n",
    "        seq += 1\n",
    "        try:\n",
    "            requests.post(INTRUDER_POST_URL, json={\n",
    "                'seq': seq,\n",
    "                'target_id': fid,\n",
    "                'active': bool(active),\n",
    "                'speed': INTRUDER_SPEED if active else 0,\n",
    "                'length': INTRUDER_LENGTH if active else 0,\n",
    "                'duration_ms': INTRUDER_DURATION_MS if active else 0,\n",
    "            }, timeout=3)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"fid={fid} | ts={sample['ts']:.2f} | pos=({sample['my_x']:.1f},{sample['my_y']:.1f}) | speed={sample['current_speed']:.3f} | gap={sample['gap']:.3f} | new_speed={spd:.3f}\")\n",
    "\n",
    "    time.sleep(0.5)\n"
   ],
   "id": "_cdBorjUqbE0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}